

Libertà di espressione, Garante privacy: "Troppo potere alle big tech, ecco come intervenire"
Intervento di Ginevra Cerrina Feroni, Vice Presidente del Garante per la protezione dei dati personali
(Agenda Digitale, 11 maggio 2021)
Prendere sul serio la libertà di espressione del pensiero non è cosa da poco. Nelle sue varie declinazioni, è la prima delle libertà – insegna dal 1859 John Stuart Mill nel capitale Saggio sulla libertà (On liberty)[1]– e connota, indefettibilmente, la forma di Stato liberaldemocratico.
Il costituzionalismo europeo del secondo dopoguerra ascrive la libertà di espressione al "nucleo duro" dei diritti inviolabili dell’uomo e, quale principio supremo dell’ordinamento, la sottrae alla revisione costituzionale. Il diritto di comunicare i propri pensieri, enunciato dalla Costituzione italiana all’art. 21, ha un raggio di azione amplissimo poiché si declina in forme specifiche, come la libertà religiosa, dell’arte e della scienza. Concerne la diffusione di opinioni, ma anche la narrazione di un fatto vero o falso che sia e abbraccia l’informazione, la critica, la satira e anche il mero gossip.
Nella libertà di espressione, vi rientra non solo il pensiero strettamente inteso, ma anche la manifestazione di mere suggestioni, emozioni, sensazioni, nonché qualsiasi attività volta a provocare corrispondenti stati d’animo o incitamento all’azione. Carlo Esposito, nel suo noto saggio del 1958, affermava che costituiscono manifestazioni del pensiero anche "la propaganda, l’apologia, la pubblica esaltazione e persino la manifestazione istigante alla realizzazione del pensiero espresso"[2].
Lo sviluppo tecnologico di questi decenni ha potenziato enormemente la libertà di espressione, consentendo la possibilità di diffondere sempre, in scala globale, il proprio pensiero[3] e ha determinato per il giurista la sfida di ripensare categorie giuridiche, linguaggi, temi, discipline[4].
Lacune di regolazione e paradossi della rete
Ovvio che l’estensione potenzialmente infinita e incontrollata della libertà di espressione nella rete rischia di favorirne anche il suo abuso e da ciò deriva l’urgenza della sua regolamentazione. A chi debba essere demandata questa regolamentazione, è la vera domanda, che aspetta ancora una risposta. Per l’ordinamento giuridico l’unico soggetto che può arrogarsi il ruolo di intermediare tra le libertà dei singoli cittadini è lo Stato; nella rete, invece, tale funzione, è esercitata in via di fatto dalle grandi piattaforme. Queste ultime non detengono soltanto il potere di decidere, ma spesso sono le sole che possiedono i dati sulla cui base poter decidere e rappresentano, pertanto, gli interlocutori necessari di ogni processo di regolazione. Ormai da molti anni, all’incirca dalla metà degli anni duemila, il sistema di assemblaggio, di interconnessione economica, tra le varie singole piattaforme ha fatto in modo che si venissero a creare imprese nelle mani delle quali si sono concentrati poteri immensi in termini di controllo dei dati degli utenti e delle informazioni scambiate.
Facebook è oggi l’‘ecosistema’ che detiene il vero controllo sostanziale sui flussi di informazioni personali degli utenti: dopo che nel 2012 ha incorporato Instagram e nel 2014 WhatsApp, attualmente detiene l’80% del mercato dei dati tra i social network. Stefano Mannoni proprio su questa testata, riprendendo l’espressione di Nicolas Petit, ha scritto che le piattaforme si pongono come un "moligopolio": monopolisti in casa, oligopolisti negli altri mercati[5], secondo un chiaro intreccio di interessi anzitutto economici, ma anche sociali e giuridici. Vengono infatti in gioco sensibilissimi profili di interesse per il diritto costituzionale, cruciali per la tenuta di un sistema democratico e, ancora in gran parte, inesplorati e quindi non consapevolizzati. Primo, fra tutti, il potere, praticamente irraggiungibile e comunque sottratto ad ogni forma di controllo democratico, delle piattaforme digitali: signori senza terra, dalla natura giuridica privata ma, proprio grazie a questa, assurti a un ruolo pubblico e totalizzante di "censori" della libertà costituzionale d’espressione. Si aggiungono i profili legati alla trasparenza delle procedure utilizzate nella repressione dei discorsi d’odio (o asseriti tali), come pure quelli legati alle motivazioni delle decisioni assunte nei confronti degli utenti, alla loro reale giustiziabilità ed ancora all’effettività delle pronunce di giustizia nei loro confronti.
I paradossi di un sistema originariamente ed intrinsecamente anarchico, eppure sviluppatosi in modo sostanzialmente autoritario, si ripropongono continuamente sotto i nostri occhi:
il sistema esprime formalmente un assetto ugualitario ma, in realtà, l’impostazione è fortemente gerarchica;
evidente l’ossimoro di un regime in cui operano imprese private le quali definiscono contenuti di diritti e libertà costituzionali e procedure per il loro godimento;
nessuna neutralità di un assetto nel quale il sistema valoriale è definito dalle piattaforme e con impronta, perlopiù, ideologicamente orientata;
la finzione di un metodo che sembra voler sostituire "dal basso" le decisioni che nei vari Stati sono considerate adottate "dall’alto" (Governo, Parlamento), mentre, al contrario, ogni decisione resa manifesta dalle piattaforme è assunta a livello apicale e centralizzato e si riversa sugli utenti senza che essi possano veramente interagire o prendere parte al processo decisorio, né tantomeno "reagire".
Le contraddizioni dell’Oversight Board di Facebook e l’ossimoro di uno Stato che abdica al suo ruolo
La scelta da parte di Facebook di istituire un Comitato di controllo, cosiddetto Oversight Board, va letta proprio come un tentativo di rimediare a questi paradossi. Essa rischia però di confermarli e, in un certo senso, addirittura di consolidarli. L’ambizione è nota: creare standard giuridici globali e farlo attraverso un organo che garantisca una posizione di terzietà. Il punto è: terzietà da chi? Dalla piattaforma (che l’ha creato), certamente, ma anche dai singoli Stati e dalle decisioni particolaristiche dei loro Tribunali. Regole di condotta e di procedura sono raccolte nei cosiddetti statuti del Comitato (Oversight Board Bylaws), che fungono tanto da atto costitutivo quanto da vero e proprio regolamento interno. Giuridicamente gli statuti permettono alle decisioni del Board di porsi come ‘sentenze’ di primo grado nei confronti delle decisioni di Facebook che hanno a che fare con la libertà di espressione degli utenti.
Tali statuti, ispirandosi in massima parte al funzionamento delle Corti internazionali, sanciscono il funzionamento del Comitato: ovvero definiscono la giurisdizione, gli atti sindacabili, la pubblicità dei lavori, i mezzi istruttori esperibili, la procedura decisoria, l’implementazione delle sue decisioni. Viene inoltre esplicitato il ruolo del precedente e della prassi tanto sotto il profilo contenutistico che sotto quello procedurale. Il Comitato sembra voler costituire un’articolazione per la governance di Internet e, in particolare, aspirare alla creazione di un patrimonio giuridico di concetti e case-law che meglio definiscano la portata e i confini della libertà di espressione nella rete. Da questo punto di vista si tratta di un’operazione ragguardevole: come potrebbero infatti giudicare uniformemente, ad esempio, un tribunale saudita o uno italiano, riguardo a un video inneggiante la lapidazione di un’adultera?
Sovrastano, tuttavia, gli aspetti problematici, che riguardano sia l’aspetto pratico, sia quello più sistematico. Sotto il primo profilo, le decisioni già cominciano a mostrare, in questo scorcio di avvio, le prime falle in termini di coerenza ed uniformità di giudizio[6]. Quanto al secondo, mai dimenticare che la creazione del Comitato è una "concessione" di Facebook e che, al di là delle apprezzabili dichiarazioni, non vi è alcun obbligo giuridico di adeguamento da parte della piattaforma né alcun meccanismo coercitivo, dal momento che, a fronte di un suo eventuale inadempimento, non sono contemplate sanzioni né procedure capaci di conseguire l "ottemperanza" delle decisioni del Comitato[7]. Più che di una limitazione, si tratta quindi piuttosto di un auto-limitazione che facilmente si trasforma in una (auto-)legittimazione della propria condotta[8].
Questo aspetto ci impone di chiederci come il diritto possa reagire, almeno negli Stati democratici, a questo tentativo da parte degli attori privati transnazionali di regolare intensamente interi settori della vita attraverso i propri regimi di governo privato[9]. Di chiederci, cioè, come può essere ridisegnata l’impostazione degli Stati-nazione in modo non forse da impedire, ma certamente da sostituire o, quantomeno, regolare questo fenomeno. Basti pensare a come soggetti che, formalmente, restano comunque privati – col prendere o lasciare, cioè col concedere o negare l’accesso alla "loro" piattaforma che pur è in posizione dominante del mercato della comunicazione social e che è divenuta strumento universale e quotidiano di interazione per le più varie dimensioni della vita (da quella sentimentale a quella professionale) – si assumano, su una base contrattuale unilaterale quanto radicalmente asimmetrica, un potere sostanzialmente pubblicistico (perché concerne la comunicazione esterna con terzi, cioè verso il pubblico). Ovvero, il potere di esercitare un penetrante controllo in relazione alle libertà di espressione del pensiero del contraente debole (l’utente, spesso sprovveduto e comunque di regola non adeguatamente informato riguardo alle funzionalità e ai meccanismi della piattaforma e tantomeno riguardo al trattamento dei suoi dati), così introducendo, a loro autonoma e non negoziabile discrezione, limitazioni e procedure ulteriori rispetto a quelli poste per tutti con la garanzia democratica e paritaria della legge.
Si tratta di una pericolosa forma di "privatizzazione della censura" e del correlato fenomeno di "privatizzazione della giustizia digitale su scala globale", dai contorni ancora incerti, pertanto ancor più rischioso e da seguire con la massima attenzione[10]. Una prospettiva aberrante, nella quale lo Stato rischia di abdicare al suo ruolo, delegando integralmente alle Internet platforms la regolazione del pluralismo informativo e il bilanciamento dei diritti fondamentali[11].
La questione centrale è se quest’ingerenza proietti un libero potere ex contractu oppure se, come riteniamo, andando ultra vires, implichi l’assunzione di capacità metacontrattuali, in sostanza pubblicistiche, e dunque incontri i limiti generali delle norme imperative e dell’ordine pubblico ideale, il quale reca con sé la latitudine ordinaria – e privatamente incensurabile! – della libertà di espressione. Limiti che occorre ritenere indefettibili e rendere operativi con l’intervento differenziato e coordinato delle competenti istituzioni pubbliche (fra cui le Autorità indipendenti, ciascuna rispetto alle rispettive attribuzioni, magari, ove necessario, in attività congiunta), perché, ragionando altrimenti, si consentirebbe l’affermazione di un potere privato quasi assoluto e comunque, fortemente ed insopportabilmente autoritario e antidemocratico.
Oltre la libertà di espressione. Quale spazio riconoscere al Garante per la protezione dei dati personali?
Inutile dire che l’emergenza pandemica sta ponendo il problema della libertà di espressione (e di informazione) in tutta la sua gravità. Si richiederà ben presto di iniziare seriamente ad affrontare l’aumento del numero di casi di sospensione/chiusura di account, collegato a opinioni politiche "alternative" o "controcorrente" rispetto all’orientamento ufficiale delle istituzioni pubbliche, sull’implicita base di una presunta "pericolosità" per l’ordinamento democratico o per la sicurezza pubblica (es. la presunta "disinformazione medica") e iniziare a costruire effettive modalità di tutela dei diritti dei cittadini rispetto all’azione e in particolare, alle limitazioni poste in essere dalle piattaforme social.
Vi è da chiedersi quale ruolo può attribuirsi al Garante per la protezione dei dati personali a fronte di tale fenomeno.
Le doglianze in astratto considerabili – e che anche in concreto sempre più emergono, in modo preoccupante, nelle interazioni fra gli utenti delle social communities – riguardano:
il mancato accesso rispetto ad account "spossessati" da hacker, oppure sospesi o chiusi, talora ex abrupto, dalle piattaforme per presunte violazioni dei "termini di servizio";
la mancanza di una motivazione effettiva, a fronte dell’asserita laconica "violazione delle regole della community" e l’impossibilità di contestare la decisione della piattaforma;
lo scollamento fra il piano delle pur scarne procedure formali (previste dalle relative policy) e quello dell’operatività concreta dei meccanismi rimediali;
la difficoltà di cancellazione dei propri account;
in via collegata, la non effettività dei riscontri della piattaforma, ove forniti, in quanto standardizzati e comunque non satisfattivi delle esigenze prospettate dagli interessati;
la perdita di esperienze, contatti, emozioni e, come espressamente (e correttamente) dicono alcuni reclamanti, di porzioni di "identità digitale";
con riguardo ad account a uso professionale/ imprenditoriale o promiscuo, danni economici collegati alla perdita di tale "vetrina" e delle interazioni verificatesi nel tempo con clienti, utenti e fornitori.
Si possono ipotizzare effetti persino paradossali come quando gli utenti, non riuscendo a ri-accedere e tantomeno a ri-appropriarsi dei propri account, si riducono a chiedere la cancellazione dell’account medesimo e dei dati ivi contenuti, talora invano. In via correlata, gli stessi non possono più ri-accedere e gestire i propri dati, ma i medesimi dati rimangono nella proprietà della sola piattaforma, ravvisandosi una pericolosa "espropriazione dei dati" (dal noto valore economico, in ragione sia delle informazioni personali sia di quelle statistiche o anonimizzate), posta in essere da tali poteri privati, che si arrogano il ruolo di "censori", oltre che di giudici di sé stessi.
Le previsioni del Gdpr
Sotto l’aspetto normativo, pur in base ad un’analisi prima facie, i parametri di riferimento diretto nel GDPR risultano chiari e ineludibili. Fra i vari: art. 5, con riferimento a limitazione della finalità e della conservazione dei dati; art. 12, anzitutto con riguardo ai principi della trasparenza (par.1) e alla necessità di un obbligo fattivo e tempestivo (par. 3); art. 13, con riguardo a tutti i contenuti necessari ai fini di un’informativa adeguata da rilasciare agli interessati; art. 22, sul divieto di decisioni automatizzate, nella triplice manifestazione del diritto di essere ascoltato, di chiedere l’intervento di un operatore umano e di contestare la decisione stessa; art. 20, relativo alla portabilità dei dati, necessaria per favorire lo sviluppo dell’economia digitale, oltre che per soddisfare le esigenze degli utenti. Norme che, nel loro insieme, mirano ad assicurare i diritti fondamentali degli interessati connessi alla protezione dei loro dati personali e, al contempo, la corretta circolazione dei medesimi dati, come attesta la denominazione stessa del GDPR.
Va poi considerata la possibile lesione del diritto all’identità digitale, come riconosciuto da tempo anche dal diritto vivente[12], e della libertà d’impresa e, più in generale, di iniziativa economica. Peraltro non può essere trascurata la dimensione civilistica dei danni espressamente contemplati dal GDPR (art. 82): danni "materiali" (come ad esempio la chiusura di account professionali, soprattutto in termini di perdita di chance) ed "immateriali" (esistenziali o persino biologici, come lesioni quantificabili e verificabili dell’integrità psicofisica, per il vulnus creatosi riguardo a vari frammenti del proprio io e/o a reti di rapporti amicali ed affettivi, oltre che alla possibilità di svilupparne di nuovi), che si pongono quali probabili conseguenze del danno-evento (violazione della normativa in materia di protezione dei dati)[13]. Danni che, come insegna anche l’orientamento più recente della Suprema Corte, occorre comunque risarcire se superano la soglia della normale tollerabilità, considerato che discendono dalla violazione di un diritto fondamentale della persona[14].
Occorre cominciare a ragionare in termini operativi di questi temi, con gli strumenti che l’ordinamento offre, poiché non sembra davvero possibile aspettare i tempi lunghi dell’approvazione dei nuovi regolamenti europei ancora in divenire, come il Digital Services Act[15], né quelli – comunque incompatibili con una tutela tempestiva ed effettiva degli interessati a fronte delle menzionate gravi violazioni – previsti dal GDPR per la procedura di cooperazione (artt. 60 ss.). Procedura, quest’ultima, funzionale ad una gestione razionale, efficiente e tendenzialmente uniforme di doglianze e problematiche, ma che non può evidentemente annullare le competenze di ciascuna Autorità, né impedire o ritardare (e quindi compromettere) la tutela dei fondamentali diritti degli interessati, perché vorrebbe dire inevitabilmente tradire la ratio stessa del GDPR, incentrata sulla persona.
E, in tale contesto, è possibile ipotizzare da parte delle DPA nazionali una verifica, ad ampio raggio, sulle attuali policies delle piattaforme e chiaramente, previo adeguato contraddittorio con le stesse, anche l’adozione dei più opportuni provvedimenti correttivi previsti dall’art. 58 del GDPR. Ciò secondo l’apprezzabile logica graduata e proporzionata, chiaramente attenta alle peculiarità quantitative e qualitative dei vari trattamenti. Intervento che peraltro risulterebbe poco invasivo, ed anzi d’ausilio alle dette finalità di cooperazione ed uniformizzazione, considerato quanto previsto dalla medesima disposizione dell’art. 66 del GDPR, la quale contempla misure di carattere solo temporaneo (trimestrale) da eventualmente adottarsi da parte dei Garanti nazionali con uno scopo prettamente "propulsore" del controllo europeo, anche alla luce del puntuale obbligo d’informazione tempestiva e circolare delle attività nazionali verso le altre DPA, il Comitato europeo per la protezione dei dati, nonché la Commissione.
 
_______
Note
(1) Si è usata l’edizione a cura di G. Mollica, J.-S. Mill, Sulla libertà, con testo inglese a fronte, Firenze, Giunti, (quarta ristampa), 2020.
(2) C. Esposito, La libertà di manifestazione del pensiero nell’ordinamento italiano (1958), in Id., Diritto costituzionale vivente (a cura di D. Nocilla), Milano, Giuffrè, 1992, 173-174.
(3) Sul tema si rinvia a T.E. Frosini, Liberté, egalité, internet, II ed., Napoli, ES, 2019.
(4) Si pensi solo alla sempre più sfumata distinzione tra comunicazione interpersonale, che intercorre tra soggetti predeterminati, caratterizzata dalla segretezza (art. 15 Cost.) e diffusione del pensiero rivolta ad incertam personam (art. 21 Cost.). Il che dipende dal fatto che con la Rete tutti possono essere al tempo stesso comunicatori e diffusori. Cfr. A. Valastro, Art. 21, in R. Bifulco, A. Celotto, M. Olivetti (a cura di), Commentario alla Costituzione, Utet, Torino, 2006, pp. 454 ss. Sul tema della libertà di espressione in rete si rinvia, per tutti, a M. Bassini, Internet e libertà di espressione, Roma, Aracne, 2019.
(5) S. Mannoni, Big tech, Ecco dove può agire l’antitrust e dove no, in Agendadigitale.eu, 1° maggio 2021.
(6) Si prenda la settima decisione, emessa il 12 febbraio 2021. Il caso riguardava un post di un utente indiano musulmano che aveva condiviso, sotto l’immagine di un uomo con un’armatura di cuoio e una spada nel fodero, la scritta "Se la lingua dell’infedele si scaglia contro il Profeta, allora la spada deve essere tirata fuori dal fodero". Facevano seguito vari hashtag che definivano il presidente francese Emmanuel Macron "il diavolo" e richiedevano il boicottaggio dei prodotti francesi. Il contenuto del post univa ciò che può essere comunemente considerato un discorso religioso (l’invocazione di Maometto) a una velata minaccia di violenza fisica basata su motivazioni politico-estremiste. Il Comitato ha deciso di revocare la rimozione decisa da Facebook. Decisivo ai fini dell’overruling, sembra essere stata l’interpretazione delle circostanze di fatto: "l’ampia portata del target (i "Kāfir") e la mancanza di chiarezza in merito al concetto di danno fisico o violenza potenziale, che non sembrava essere imminente, hanno contribuito – secondo l’opinione del Comitato – alla decisione finale della maggioranza". Cfr. il testo su https://oversightboard.com/decision/FB-R9K87402/. Tuttavia tale decisione non ha tenuto conto di molti elementi che in altre decisioni erano state giudicati essenziali, come: la circostanza che il termine fosse esplicitamente presente in una lista di oltraggi vietati negli standard di community, che l’incitamento all’odio fosse supportato da un segno materiale e esplicito (in questo caso, la spada); che il contenuto, condiviso decine di migliaia di volte, fosse stato pubblicato in un periodo di tensioni religiose nel Paese. È bene sottolineare come tale decisione si ponga in aperta, ed irragionevole, contraddizione con il decisum relativo ad un precedente caso speculare (decisione 2020-003-FB-UA).
(7) È ciò che viene evidenziato anche da v. K. Klonick, The Facebook Oversight Board: Creating and Indipendent Institution to Adjudicate Online Copyright, in 129 Yale L. J., 2019-2020, 2418 ss.
(8) Interessante anche la tesi di J. Franchi, Oversight Board, chi controlla il controllore? Tutti i dubbi sul "tribunale supremo" di Facebook, Agendadigitale.eu, 6 aprile 2021, che vedrebbe l’Oversight Board non come un organo di difesa degli utenti ma piuttosto come tribunale interno di supervisione del lavoro dei moderatori di contenuti.
(9) Sui presupposti teorici del problema, cfr. A. Gatti, Istituzione e anarchia nella rete. I paradigmi tradizionali della sovranità alla prova di Internet, in Diritto dell’Informazione e dell’Informatica, n. 3/2019, 711-743.
(10) v. O. Pollicino, L’"autunno caldo" della Corte di giustizia in tema di tutela dei diritti fondamentali in rete e le sfide del costituzionalismo alle prese con i nuovi poteri privati in ambito digitale, in Federalismi.it, 16 ottobre 2019, n. 19.
(11) Sul tema, in relazione al tema delle fake news, da ultimo, M. Monti, La disinformazione online, la crisi del rapporto pubblico-esperti e il rischio della privatizzazione della censura nelle azioni dell’Unione Europea (Code of practice on disinformation), in Federalismi, 11/2020, 15 ss.
(12) Cfr., in quest’ottica già Corte cass. 5 aprile 2012, n. 5525, riguardante – al di là del caso di specie – la necessità di una completa e corretta rappresentazione della vicenda processual-penalistica apparsa in origine sui giornali riguardo all’avvio della stessa e quindi da pubblicizzare dalle medesime testate, avendo riguardo al suo sviluppo favorevole all’indagato/imputato (assoluzione), e così anche alla storia digitale dell’interessato.
(13) Utile quanto rilevante in proposito è il considerando 85 del GDPR, che, nell’elenco esemplificativo dei possibili danni conseguenti dalla violazione dei dati personali, ha inserito la macrocategoria della "limitazione di un proprio diritto" oltre che specifici tipi di danno, quali "le perdite finanziarie", certamente ravvisabili anche con riguardo alle limitazioni poste in essere dalle piattaforme verso i propri utenti.
(14) In particolare, v. la recente ordinanza n. 17383/2020 della S.C., che, con riguardo alle disposizioni degli artt. 11 e 15 del d.lgs. n. 196 del 2003 (abrogate dal d.lgs. n. 101/2018, ma riprese dall’art. 82 del GDPR), nega il risarcimento in re ipsa del danno per il solo fatto del trattamento dei dati personali. Tuttavia riconosce espressamente la risarcibilità del danno non patrimoniale determinato da una lesione del diritto fondamentale alla protezione dei dati personali tutelato dagli artt. 2 e 21 Cost. e dall’art. 8 della CEDU, previa "verifica della ‘gravità della lesione’ e della ‘serietà del danno’ (quale perdita di natura personale effettivamente patita dall’interessato), in quanto anche per tale diritto opera il bilanciamento con il principio di solidarietà ex art. 2 Cost., di cui il principio di tolleranza della lesione minima è intrinseco precipitato". Per un commento di tale ordinanza, v. P. Montella, Illecito trattamento dei dati, come valutare se si ha diritto a un risarcimento, in www.cybersecurity360.it).
(15) In tale ottica, risultano sicuramente pertinenti ed utili più disposizioni del DSA che potremmo definire ad impatto "strutturale" rispetto alle politiche e alle prassi delle piattaforme. Anzitutto, si veda l’art. 12, sull’obbligo di predisporre condizioni generali che dovranno descrivere anche "le politiche, le procedure, le misure e gli strumenti utilizzati ai fini della moderazione dei contenuti, compresi il processo decisionale algoritmico e la verifica umana (…) redatte in un linguaggio chiaro e privo di ambiguità e …disponibili al pubblico in un formato facilmente accessibile"; nonché sul correlato obbligo attuativo di tali policies "in modo diligente, obiettivo e proporzionato…. tenendo debitamente conto dei diritti e degli interessi legittimi di tutte le parti coinvolte, compresi i diritti fondamentali sanciti dalla Carta". Si segnalano anche: l’art. 13, che stabilisce l’obbligo delle piattaforme di pubblicare, con cadenza almeno annuale, una relazione su elementi quantitativi e qualitativi delle attività di moderazione e limitazione degli account e degli altri servizi, con riferimento non solo alle segnalazioni di eventuali abusi pervenute da altri utenti ma anche ad iniziative autonomamente assunte dalle piattaforme; l’art. 14, sull’obbligo di "motivazione" articolata ed adeguata delle decisioni limitative adottate; l’art. 17, che prevede l’obbligo per le piattaforme di garantire "per un periodo di almeno sei mesi dalla decisione l’accesso a un sistema interno di gestione dei reclami efficace, che consenta di presentare per via elettronica e gratuitamente reclami contro le decisioni adottate". Ad esempio rimuovere le informazioni o disabilitare l’accesso alle stesse; sospendere o cessare l’account dei destinatari; predisporre sistemi interni di gestione dei reclami di facile accesso e uso. V. anche art. 20, che codifica un vero e proprio procedimento garantistico di valutazione (di eventuali abusi del diritto di segnalazione di contenuti illegali e violazioni dei termini di servizio), attenta ed agganciata a precisi elementi: "1.Dopo aver emesso un avviso preventivo, le piattaforme online sospendono per un periodo di tempo ragionevole la prestazione dei loro servizi ai destinatari del servizio che con frequenza forniscono contenuti manifestamente illegali. 2. Dopo aver emesso un avviso preventivo, le piattaforme online sospendono per un periodo di tempo ragionevole il trattamento delle notifiche e dei reclami presentati mediante i meccanismi di notifica e azione e i sistemi interni di trattamento dei reclami (…) da persone, enti o reclamanti che con frequenza presentano notifiche o reclami manifestamente infondati. 3. Le piattaforme online valutano, caso per caso e in modo tempestivo, diligente e obiettivo, se un destinatario, una persona, un ente o un reclamante commetta abusi (…), tenendo conto di tutti i fatti e di tutte le circostanze pertinenti che risultano dalle informazioni a disposizione della piattaforma online. Tali circostanze comprendono almeno quanto segue: a) il numero, in termini assoluti, di contenuti manifestamente illegali o di notifiche o reclami manifestamente infondati presentati durante l’anno precedente; b) la relativa proporzione rispetto al numero totale di informazioni fornite o di notifiche presentate nell’anno precedente; c) la gravità degli abusi e delle relative conseguenze; d) l’intenzione del destinatario, della persona, dell’ente o del reclamante".




Feroni (GPDP): “IA nei processi decisionali della PA, il faro è la Costituzione”
Il tema dell’intelligenza artificiale ha a che fare con la democrazia. in primis perché il potere oggi si concretizza digitalmente, nel detenere di una persona tutto ciò che essa ha di più prezioso: ovvero i suoi dati. Il costituzionalismo nasce proprio per porre limiti al potere
Intervento di Ginevra Cerrina Feroni, Vice presidente del Garante per la protezione dei dati personali
(AgendaDigitale, 29 maggio 2023) 
Quando parliamo di intelligenza artificiale è cruciale sottolineare che siamo solo all’inizio di una storia che deve ancora essere scritta. E il momento per cominciare a scriverla è adesso. Solo considerando cosa c’è in gioco, già adesso, quando si parla di IA.
IA, una sfida (anche) antropologica ed etica
Emblematico che sia stato lo stesso CEO di Open AI, Sam Altman, a dichiarare che: “L’intelligenza artificiale va messa sotto controllo. Potenzialmente è pericolosa come le armi nucleari”. Una ammissione che in un certo senso rende giustizia di una narrativa, invero un po’ stereotipata, secondo la quale il concetto stesso di progresso tecnologico meriterebbe una adesione acritica e incondizionata e che sarebbe irrealistico ipotizzare che esso possa essere regolato, indirizzato, diretto.
Il tema è enorme perché coinvolge una pluralità di profili: la sfida dell’IA non è soltanto di natura infrastrutturale, ma anche antropologica ed etica.
Antropologica, perché l’uso di strumenti di intelligenza artificiale rinvia necessariamente al rapporto tra l’uomo e la tecnologia, evocando la possibilità di sostituire il primo con una macchina che è così umanizzata da essere persino costruita con le sembianze umane o, in ogni caso, con programmi che la fanno muovere e decidere come se si trattasse di un essere vivente. E poi etica. Anche se personalmente preferisco parlare di dimensione costituzionale dell’intelligenza artificiale[1], essendo tipico proprio delle Costituzioni tradurre principi etici in disposizioni normative ed operare un bilanciamento fra valori differenti.
La relazione tra intelligenza artificiale e costituzionalismo
Dimensione costituzionale perché ha a che fare col potere, pubblico o privato che sia. Col potere della sorveglianza fatta di telecamere intelligenti, di riconoscimenti biometrici, di profilazioni profonde e senza scampo non solo di preferenze musicali o di luoghi di villeggiatura, ma di pensieri, desideri, emozioni, paure, ovvero di ciò che è l’identità più profonda e, teoricamente, segreta di ognuno di noi.
Se è vero che potere è sapere, nulla è veramente cambiato nella storia, sono mutate solo le condizioni in cui esso si esercita.
E il potere oggi si esercita, digitalmente, nel fatto di detenere di una persona tutto ciò che di più prezioso quella persona ha: ovvero i suoi dati, cioè la sua identità. Disporre di quei dati significa disporre delle informazioni sulla sua salute, sul suo orientamento politico, sulle sue preferenze sessuali, sulla sua situazione finanziaria, sulla sua storia, insomma sulla sua vita.
Il costituzionalismo nasce proprio per porre limiti al potere, o come pure si dice come “tecnica delle libertà”. Se non si capisce il contesto di riferimento, si rischia di perdere il senso più profondo dell’impatto delle tecnologie nelle società democratiche.
Il tema della intelligenza artificiale ha, dunque, davvero a che fare con la democrazia. Non riesco a leggere diversamente l’impatto che l’IA può avere sulle funzioni tradizionalmente esercitate dallo Stato e sulla trama di diritti e libertà costituzionalmente riconosciuti ai consociati: difesa, erario, salute, istruzione, lavoro, libertà di impresa ecc. Tutto ne risulta profondamente trasformato.
Ora, essendo l’IA una dimensione complessa, il miglior modo per ricavare dei principi-chiavi è proprio quello di guardare alle opportunità, ovvero il grande contributo che essa apporta allo sviluppo economico e sociale del Paese. Senza sottovalutare i rischi come i meccanismi decisionali opachi o incontrollabili (si pensi agli algoritmi machine learning il cui reasoning non è conosciuto nemmeno dai loro progettatori), le errate configurazioni, o le fughe di dati e gli hackeraggi che possono ledere diritti fondamentali individuali e collettivi e mettere in pericolo la sicurezza del Paese.
Certo, nel tempo sono stati enucleati alcuni assiomi-cardine che devono orientare gli operatori. Sul tema ad esempio della giustizia. si deve ricordare la Carta etica sull’utilizzo dell’intelligenza artificiale nei sistemi giudiziari e ambiti connessi, redatta dalla Commissione europea per l’efficienza della giustizia (2018) in cui si sono stati elaborati alcuni concetti che poi sono divenuti la base per la regolazione. Tra questi: la qualità e neutralità dei dati; la responsabilità di chi progetta e utilizza gli algoritmi; la trasparenza di tali algoritmi e, ovviamente, tutela della privacy[2].
Tuttavia, non è tanto l’individuazione dei principi e delle regole astratte da applicare alla IA ad essere problematica, quanto la loro concreta applicazione. È necessario cioè capire come questa normativa andrà a recepire principi e regole nella vita sia digitale che analogica regolata dall’IA.
Il principio della privacy by design per un’IA “fundamental-rights oriented”
È ovvio che alla base di tutto debba esserci l’interpretazione “fundamental-rights oriented”[3], asse portante del GDPR[4]. In questa prospettiva, il principio che maggiormente sembra interpretare l’esigenza di integrare il rispetto dei diritti fondamentali nello sviluppo tecnologico è quello della privacy by design enunciato nell’articolo 25 del GDPR che può essere definita come la protezione dei dati personali fin dal momento in cui un sistema di IA (una app, un software, una macchina intelligente…) viene progettato[5].
Il Considerando 78 GDPR è a tal fine esplicativo quando afferma, tra le altre cose, che lo sviluppo, la progettazione, la selezione e l’utilizzo di applicazioni dovrebbero essere incoraggiati a tenere conto del diritto alla protezione dei dati e dei suoi presupposti e precipitati. Ciò significa evidentemente inserire una componente valoriale all’interno della progettazione tecnica e questo rende l’articolo 25 un precetto di tenore “quasi costituzionale”. Costituzionale in senso stretto, poiché rappresenta un requisito fondante della costruzione e del funzionamento dei processi algoritmici nella realtà sociale digitale; costituzionale in senso ampio perché inserisce a pieno titolo i diritti costituzionali nella dimensione digitale.
Esercizio della funzione amministrativa e utilizzo dell’intelligenza artificiale
Il profilo che merita di essere indagato in questa sede è sicuramente quello che attiene al rapporto tra esercizio della funzione amministrativa e utilizzo dell’IA[6].
È infatti fenomeno diffuso, anche nell’ambito dell’agire pubblico, quello secondo cui un numero sempre maggiore di decisioni, tra cui anche quelle capaci di incidere negativamente sulle libertà dei singoli, sono prese o supportate da macchine che di fatto si sostituiscono all’uomo.
Non si può fare a meno di sottolineare, nell’ottica pubblicistica, che realtà come quelle evocate si pongano in evidente contraddizione con i postulati risalenti del diritto pubblico che sono stati elaborati e codificati partendo dall’idea che la persona umana sia non solo la destinataria, ma anche l’artefice delle azioni pubbliche, con ciò che esso implica in termini di valutazioni, di scelte, di responsabilità.
Sono molte le categorie concettuali del diritto pubblico che vengono intercettate dall’irrompere nell’azione dei pubblici poteri dell’intelligenza artificiale; basti ricordare la teoria dell’organo, come individuazione del soggetto cui compete la scelta decisionale, la tematica della responsabilità, la discrezionalità amministrativa, con l’ambito di scelte ad essa connesse[7], ovvero richiamare le dottrine della ragionevolezza, proporzionalità o del bilanciamento degli interessi nell’ambito del diritto costituzionale o amministrativo[8].
Tutti istituti che postulano una riconduzione alla responsabilità del soggetto agente, ad una sua ponderazione, e presuppongono una piena conoscenza dei percorsi valutativi posti in essere dai soggetti agenti e una conseguente loro sindacabilità in sede giudiziaria.
Senza dire della più ampia sistematica delle libertà individuali, della loro tutela, delle loro garanzie. Riemerge, né poteva essere diversamente, la problematica di fondo, ovvero il ruolo dell’uomo e l’individuazione di una vera e propria “riserva di umanità” per citare la recente monografia di Giovanni Gallone[9]. La problematicità dell’evoluzione in atto è stata colta già nelle prime pronunce del giudice amministrativo, che ha stigmatizzato la contrarietà ai principi di diritto amministrativo dell’integrale affidamento del procedimento amministrativo a processi tecnologici, perché ciò determinerebbe “il disimpegno di attività istruttoria, acquisitiva di rappresentazioni di circostanze di fatto e situazioni personali degli interessati destinatari del provvedimento finale; attività, talora ponderativa e comparativa di interessi e conseguentemente necessariamente motivazionale, che solo l’opera e l’attività dianoetica dell’uomo può svolgere”[10].
Lungo tutto il dettato della Costituzione si evidenzia un’assoluta centralità della persona umana, portato diretto del compromesso raggiunto, in Assemblea costituente, tra le tradizioni politiche e costituzionali delle sinistre, del pensiero liberale e democratico e di quello cattolico. La enucleata riserva di umanità rappresenta, all’evidenza, uno dei diversi corollari del principio personalista che innerva il nostro ordinamento e che risulta sancito all’art. 2 Cost., letto congiuntamente a quello della dignità della persona umana[11].
Da qui il problema se sottoporre la persona alla decisione integralmente automatizzata di un software, risulti in sé lesivo della dignità della persona stessa. Non si tratta di assumere posizioni di contrarietà agli sviluppi tecnologici, forieri sicuramente del migliore perseguimento di interessi pubblici e, quindi, funzionali al buon andamento della p.a. di cui all’art. 97 Cost. Ma di porre in evidenza, già partendo dal quadro costituzionale, che la tutela della dignità della persona richiede che l’intervento dell’uomo nel compimento della scelta amministrativa non sia del tutto obliterato o marginalizzato, proprio per rispettare la gerarchia assiologica voluta dalla Costituzione e alzare un argine alla “disumanizzazione” dell’amministrazione, riportando la macchina a semplice strumento dell’azione amministrativa[12].
Anche lo statuto del procedimento amministrativo è incentrato sull’uomo. L’elemento personalistico non è solo al centro della disciplina costituzionale, ma assume primaria importanza nel contesto della disciplina delle funzioni amministrative, nel definire le funzioni amministrative.
È vero che l’art. 3-bis della L. n. 241 del 1990 apre l’attività amministrativa all’uso della telematica (nelle sue diverse manifestazioni e, quindi, anche nelle forme dell’informatica decisionale)[13], ma da esso si ricava l’idea che quelli informatici e telematici siano “strumenti”, rispetto al momento decisionale che è ancorata in mano al funzionario[14]. Si pensi alle due figure centrali nella dinamica del procedimento amministrativo che sono il responsabile del procedimento (signore dell’istruttoria) e l’organo competente all’adozione del provvedimento finale (normalmente il dirigente). Nella sistematica della legge sul procedimento amministrativo si tratta di attori di importanza fondamentale del progressivo farsi dell’atto finale, soggetti ai quali spettano sfere di competenza e di responsabilità che risultano non espropriabili ad opera di meccanismi tecnologici, così che questi ultimi dovranno essere costruiti come strumenti nelle loro mani e attraverso i quali i soggetti responsabili esplicano i propri compiti e si assumono le correlate responsabilità.
L’art. 41 del C.A.D.[15], d’altra parte, stabilisce che il fascicolo informatico rechi l’indicazione anche “del responsabile del procedimento”, confermando che l’informatizzazione dell’azione amministrativa non può mai tradursi nella completa estromissione dell’attività umana, ma deve piuttosto risultare con essa coerente e anzi implementare la potenzialità di azione dei soggetti pubblici, nello svolgimento delle istruttorie ovvero nell’assunzione delle determinazioni finali dei procedimenti, di scelta tra le varie alternative possibili e legittime. È infatti al responsabile del procedimento che compete il governo dell’istruttoria amministrativa ed è al responsabile del provvedimento che compete l’assunzione della decisione finale, con il carico di scelte, valutazioni, ponderazioni di interessi a ciò connesso.
Certo, l’amministrazione può sempre optare per lo svolgimento in forma automatica del procedimento, come manifestazione del potere generale (ai sensi dell’art. 6 comma 1 lett. b della L. n. 241 del 1990), del responsabile del procedimento di definire le modalità di svolgimento dello stesso. Ma tale scelta non esautora gli attori del procedimento e non si traduce in una abdicazione dalle loro competenze e dai loro poteri. Ed infatti la disciplina del procedimento amministrativo offre lo schema legale per comporre il momento digitale e l’insopprimibile momento umano, nell’ambito della disciplina dell’art. 6 della legge n. 241 del 1990.
Il prodotto dell’operazione algoritmica automatizzata non rappresenta altro che le “risultanze dell’istruttoria” da porre a base dell’adozione del provvedimento finale ai sensi dell’art. 6, comma 1, lett. e) della l. 7 agosto 1990, n. 241. Il funzionario persona fisica può fare proprio il risultato computazionale adeguandosi allo stesso e recependolo in seno al provvedimento; in alternativa, il funzionario medesimo può scegliere di discostarsi dal prodotto dell’operazione algoritmica e provvedere in maniera diversa. Il discostarsi in sede decisoria dalle risultanze dell’istruttoria richiede, come risulta dalla disciplina citata, un’adeguata motivazione, la quale costituisce lo strumento tecnico attraverso il quale il fattore umano (cioè il funzionario responsabile) supera il risultato computazionale prodotto dell’automazione. Sarà la prassi applicativa ad evidenziare la casistica in cui ciò può avvenire; si può ipotizzare che il funzionario faccia valere un errore di computo in senso stretto; ovvero un errore materiale commesso in sede di input, nonché inerente alla correttezza della scelta a monte (cioè in sede di volizione preliminare con cui si opta per l’automazione), dell’algoritmo da impiegare; ma deve anche evidenziarsi la possibilità che il funzionario faccia valere la manifesta ingiustizia, o la illogicità o la erroneità del risultato cui è pervenuta la componente tecnologica, con superamento quindi di quell’inaccettabile esito digitale del procedimento.
Gli approcci della giurisprudenza amministrativa all’intelligenza artificiale
Nella giurisprudenza [16], com’era inevitabile, sono emerse sensibilità diverse. Da una parte c’è la diffidenza di quella parte degli interpreti che esprime preoccupazioni, non certo irragionevoli, per quella che il Tar Lazio definisce, con un’immagine invero forte, ma evocativa di una lettura molto netta delle funzioni, “l’abdicazione da parte del responsabile del procedimento del suo ruolo di timoniere ed anche di correttore”[17] del procedimento amministrativo.
Nell’ambito di questa linea di pensiero, si distingue qualche posizione più avanzata di, pur timida, apertura in cui si suggerisce di utilizzare gli algoritmi tradizionali, ovvero l’intelligenza artificiale limitatamente alle attività vincolate, escludendo quindi l’impiego della stessa solo nei procedimenti discrezionali. Secondo altre opinioni l’impiego dell’intelligenza artificiale potrebbe avvenire solo in ipotesi di attività amministrativa connotate da assoluta ripetibilità delle fattispecie esaminate, in cui quindi l’apporto umano è pressoché ininfluente: una posizione di lettura classica della macchina come mero supporto dell’uomo nelle azioni faticose e ripetitive – appunto meccaniche – che assicura precisione, rapidità e sollievo.
Esiste però anche un’altra tendenza nella giurisprudenza amministrativa che invece accetta l’esistenza degli algoritmi, ne ammette l’utilizzazione nell’azione amministrativa e prova piuttosto a contenerne l’operato a tutela dei diritti degli individui che possono essere investiti. Afferma il Consiglio di Stato che non “vi sono ragioni di principio, ovvero concrete, per limitare l’utilizzo all’attività amministrativa vincolata piuttosto che discrezionale, entrambe espressione di attività autoritativa svolta nel perseguimento del pubblico interesse” (in termini Cons. Stato, sez. VI, 13 novembre 2019, n. 8472; Cons. Stato, sez. VI, 4 febbraio 2020, n. 881).
Si tratterebbe piuttosto di un problema di limiti a garanzia dei diritti fondamentali. E i limiti individuati dalla giurisprudenza corrono paralleli a quelli individuati dal Regolamento europeo sulla protezione dei dati personali. Sono i limiti della “conoscibilità”, della “non esclusività” e della “non discriminazione.
Qui va richiamata la fondamentale sentenza del Cons. Stato, sez. IV, 13 dicembre 2019, n. 8472, la quale disegna lo statuto generale delle funzioni amministrative automatizzate intorno ai tre principi fondamentali.
Quanto al principio di conoscibilità, esso si declina con il diritto di ognuno “a conoscere l’esistenza di processi decisionali automatizzati che lo riguardino ed in questo caso a ricevere informazioni significative sulla logica utilizzata” (Cons. St., sez. VI, 13 dicembre 2019, n. 8474, cit., par. 15.1)[18]. Tale principio si completa con il principio di comprensibilità, ovverosia la possibilità, per riprendere l’espressione del GDPR art. 15, par. 1, lett. h) Regolamento, di ricevere «informazioni significative sulla logica utilizzata».
Qui si intercetta un problema centrale, ovvero quello della tracciabilità dei processi decisionali, soprattutto nel caso di algoritmi a struttura aperta i quali scontano, per loro natura, un certo grado di opacità[19]. Si pensi alle fattispecie in cui, pur conoscendo per intero la sequenza base, non è possibile stabilire (neppure da parte dello sviluppatore del software) quale sarà il percorso logico seguito dall’elaboratore: una parte di esso rimarrà in ombra e sfuggirà al dominio (e al controllo) del destinatario della decisione adottata sulla base del risultato computazionale finale.
Ecco che si pone la questione di come il principio di conoscibilità-comprensibilità richiesto dal diritto euro-nazionale possa trovare applicazione nel caso di algoritmi strutturati secondo logiche di natura non deterministica.
Il principio di “non discriminazione algoritmica” è formulato partendo dal considerando 71 del GDPR. Esso prescrive che “il titolare del trattamento utilizzi procedure matematiche o statistiche appropriate per la profilazione, mettendo in atto misure tecniche e organizzative adeguate al fine di garantire, in particolare, che siano rettificati i fattori che comportano inesattezze dei dati e sia minimizzato il rischio di errori e al fine di garantire la sicurezza dei dati personali, secondo una modalità che tenga conto dei potenziali rischi esistenti per gli interessi e i diritti dell’interessato e che impedisca tra l’altro effetti discriminatori nei confronti di persone fisiche sulla base della razza o dell’origine etnica, delle opinioni politiche, della religione o delle convinzioni personali, dell’appartenenza sindacale, dello status genetico, dello stato di salute o dell’orientamento sessuale, ovvero che comportano misure aventi tali effetti” (Cons. St., sez. VI, 13 dicembre 2019, n. 8474, cit., par. 15.3).
Infine va richiamato il principio di “non esclusività della decisione algoritmica”[20]. È d’obbligo il richiamo all’art. 22 GDPR: nel caso in cui una decisione automatizzata “produca effetti giuridici che riguardano o che incidano significativamente su una persona”, questa ha diritto a che tale decisione non sia basata unicamente su tale processo automatizzato. Quindi deve comunque esistere nel processo decisionale un contributo umano capace di controllare, validare ovvero smentire la decisione automatica.
L’iter motivazionale della sentenza n. 8472 cit. muove dall’esigenza di garantire “l’imputabilità della decisione all’organo titolare del potere, il quale deve poter svolgere la necessaria verifica di logicità e legittimità della scelta e degli esiti affidati all’algoritmo”.
Ad avviso del Collegio l’individuazione del “soggetto responsabile” della scelta si impone, peraltro, “sia nell’interesse della stessa p.a. che dei soggetti coinvolti ed incisi dall’azione amministrativa affidata all’algoritmo”.
Si può certamente sostenere che il Consiglio di Stato non ha fatto mancare il suo prezioso apporto, contribuendo ad elaborare una sistematica di principi regolatori, nel dialogo con la dottrina, in assenza di norme positive, riuscendo a tradurre in disciplina normativa espressa i richiamati principi, in sede di elaborazione dello schema del nuovo codice dei contratti pubblici (su cui infra).
Anche in quest’occasione, dunque, il Consiglio di Stato ha giocato un ruolo decisivo, nella intersezione tra i vari formanti giuridici, quale vero e proprio costruttore e artefice di diritto.
Prime applicazioni legislative: il nuovo Codice dei contratti pubblici
A livello nazionale, in un ambito settoriale ma di primaria importanza, deve essere appunto evidenziata la recente normativa sui contratti pubblici[21] (d. lgs. n. 36 del 2023) dove le regole sulla legalità algoritmica, così come emerse dalla riflessione dottrinale e dalla elaborazione giurisprudenziale, hanno trovato una prima codificazione. Il riferimento è all’art. 30 del Codice che così dispone:
“1. Per migliorare l’efficienza le stazioni appaltanti e gli enti concedenti provvedono, ove possibile, ad automatizzare le proprie attività ricorrendo a soluzioni tecnologiche, ivi incluse l’intelligenza artificiale e le tecnologie di registri distribuiti, nel rispetto delle specifiche disposizioni in materia.
2. Nell’acquisto o sviluppo delle soluzioni di cui al comma 1 le stazioni appaltanti e gli enti concedenti:
a) assicurano la disponibilità del codice sorgente, della relativa documentazione, nonché di ogni altro elemento utile a comprenderne le logiche di funzionamento;
b) introducono negli atti di indizione delle gare clausole volte ad assicurare le prestazioni di assistenza e manutenzione necessarie alla correzione degli errori e degli effetti indesiderati derivanti dall’automazione.
3. Le decisioni assunte mediante automazione rispettano i principi di:
a) conoscibilità e comprensibilità, per cui ogni operatore economico ha diritto a conoscere l’esistenza di processi decisionali automatizzati che lo riguardino e, in tal caso, a ricevere informazioni significative sulla logica utilizzata;
b) non esclusività della decisione algoritmica, per cui comunque esiste nel processo decisionale un contributo umano capace di controllare, validare ovvero smentire la decisione automatizzata;
c) non discriminazione algoritmica, per cui il titolare mette in atto misure tecniche e organizzative adeguate al fine di impedire effetti discriminatori nei confronti degli operatori economici.
4. Le stazioni appaltanti e gli enti concedenti adottano ogni misura tecnica e organizzativa atta a garantire che siano rettificati i fattori che comportano inesattezze dei dati e sia minimizzato il rischio di errori, nonché a impedire effetti discriminatori nei confronti di persone fisiche sulla base della nazionalità, dell’origine etnica, delle opinioni politiche, della religione, delle convinzioni personali, dell’appartenenza sindacale, dei caratteri somatici, dello status genetico, dello stato di salute, del genere o dell’orientamento sessuale.
5. Le pubbliche amministrazioni pubblicano sul sito istituzionale, nella sezione «Amministrazione trasparente», l’elenco delle soluzioni tecnologiche di cui al comma 1 utilizzate ai fini dello svolgimento della propria attività”.
È evidente come il testo normativo (che è stato elaborato da una Commissione istituita presso il Consiglio di Stato e incaricata di predisporre il testo del decreto legislativo, ai sensi della legge delega 21 giugno 2022, n. 78) fa tesoro della elaborazione giurisprudenziale sopra descritta e costituisce una prima codificazione, in norma positiva, dei principi emersi dalla prassi giudiziaria. Si tratta, è vero, di una norma settoriale; ma essa potrà essere ulteriore base di elaborazioni interpretative e nuova evoluzione giurisprudenziale, anche in settori esterni a quello della contrattualistica pubblica.
L’atteso Regolamento europeo sull’intelligenza artificiale
Un approdo importante, al fine di porre un primo nucleo di principi normativi di governo del fenomeno in esame, sarà quello, atteso, del Regolamento UE sull’intelligenza artificiale. Si coglie nella proposta europea la ricerca di un punto di equilibrio tra contrapposte esigenze, in particolare tra il non impedire le innovazioni ma nel contempo renderle compatibili con il sistema europeo di tutela dei diritti[22].
È significativo che nell’indicare gli obiettivi della regolazione l’accento venga per primo posto sull’esigenze di “assicurare che i sistemi di IA immessi sul mercato dell’Unione e utilizzati siano sicuri e rispettino la normativa vigente in materia di diritti fondamentali e i valori dell’Unione”.
A questo primo obiettivo se ne aggiungono gli altri, che devono coordinarsi col primo: “assicurare la certezza del diritto per facilitare gli investimenti e l’innovazione nell’intelligenza artificiale”, “migliorare la governance e l’applicazione effettiva della normativa esistente in materia di diritti fondamentali e requisiti di sicurezza applicabili ai sistemi di IA”, “facilitare lo sviluppo di un mercato unico per applicazioni di IA lecite, sicure e affidabili nonché prevenire la frammentazione del mercato”.
In una logica correlata al principio di proporzionalità, si individuano – come noto – livelli di rischio diversificati, cioè inaccettabile, alto ovvero minimo (qui solo obblighi di informazione) correlando a ciascun livello un regime giuridico di regolazione dell’utilizzo dell’IA. L’obiettivo di garanzia dei diritti fondamentali in presenza di un rischio inaccettabile per i diritti stessi impone la qualificazione, nell’ambito del titolo II della proposta, delle “pratiche di intelligenza artificiale vietate”; non vi è margine di equilibrio e deve disporsi il divieto di utilizzo. Esso sono individuate dall’art. 5: ad es. sistemi di IA che utilizzano pratiche subliminali o sfruttano la vulnerabilità delle persone.
Alcuni divieti correlati all’utilizzo dell’IA da parte delle autorità pubbliche
Nel quadro della nostra odierna riflessione sono interessanti alcuni divieti correlati all’utilizzo dell’IA da parte delle autorità pubbliche.
l’utilizzo di sistemi di IA “ai fini della valutazione o della classificazione dell’affidabilità delle persone fisiche” con attribuzione alle stesse di un “punteggio sociale” utilizzato per attribuire loro “un trattamento pregiudizievole o sfavorevole” “in contesti sociali che non sono collegati ai contesti in cui i dati sono stati originariamente generati o raccolti” e/o comunque “ingiustificato o sproporzionato rispetto al loro comportamento sociale o alla sua gravità” (c.d. ranking delle persone):
l’uso di sistemi di identificazione biometrica remota “in tempo reale” in spazi accessibili al pubblico a fini di attività di contrasto, a meno che e nella misura in cui tale uso sia strettamente necessario per uno dei seguenti obiettivi: la ricerca mirata di potenziali vittime specifiche di reato, compresi i minori scomparsi; la prevenzione di una minaccia specifica, sostanziale e imminente per la vita o l’incolumità fisica delle persone fisiche o di un attacco terroristico; l’identificazione di soggetti di reati ritenuti di particolare gravità. In ogni caso l’utilizzo della identificazione biometrica reale “è subordinato a un’autorizzazione preventiva rilasciata da un’autorità giudiziaria o da un’autorità amministrativa indipendente dello Stato membro in cui deve avvenire l’uso”, dopo che questa abbia accertato che tale uso è necessario e proporzionato.
Al secondo livello abbiamo i sistemi di IA ad alto rischio. Qui il regime giuridico è più complesso e si articola nei seguenti passaggi:
“sistema di gestione dei rischi” (art. 9): è obbligatorio istituire un sistema di gestione dei rischi che identifica i rischi associati a ciascun sistema di IA ad alto rischio; li stima e valuta; adotta adeguate misure di gestione dei rischi. Il par. 8 dell’art. 9 prevede che nell’attuare il sistema di gestione dei rischi è prestata particolare attenzione all’eventualità che il sistema di IA ad alto rischio “sia accessibile ai minori o abbia un impatto su di essi”;
garanzia della qualità dai dati (art. 10): soprattutto nei sistemi di machine learning, che prevedono l’utilizzo dei dati per l’addestramento del sistema, la qualità accuratezza e completezza dei dati utilizzati sono essenziali per minimizzare la possibilità di errori e la distorsione da parte dei sistemi di IA; l’art. 10 detta regole sulla governance dei dati e sugli strumenti per garantire la bontà degli stessi,
trasparenza (art. 13): “I sistemi di IA ad alto rischio sono progettati e sviluppati in modo tale da garantire che il loro funzionamento sia sufficientemente trasparente da consentire agli utenti di interpretare l’output del sistema e utilizzarlo adeguatamente”;
sorveglianza umana” (art. 14): il par. 1 afferma che “I sistemi di IA ad alto rischio sono progettati e sviluppati, anche con strumenti di interfaccia uomo-macchina adeguati, in modo tale da poter essere efficacemente supervisionati da persone fisiche durante il periodo in cui il sistema di IA è in uso”; e il par. 2 aggiunge che “la sorveglianza umana mira a prevenire o ridurre al minimo i rischi per la salute, la sicurezza o i diritti fondamentali che possono emergere quando un sistema di IA ad alto rischio è utilizzato conformemente alla sua finalità prevista o in condizioni di uso improprio ragionevolmente prevedibile, in particolare quando tali rischi persistono nonostante l’applicazione di altri requisiti di cui al presente capo”. In proposito è stato osservato che “le misure adottate per la supervisione umana devono di conseguenza essere tali da assicurare che il supervisore umano possa autonomamente decidere di non utilizzare il sistema, di non aderire ai risultati e possa addirittura interrompere il funzionamento”[23].
Resta il nodo cruciale di cui all’art. 59 del progetto di Regolamento secondo il quale “ciascuno Stato membro istituisce o designa autorità nazionali competenti al fine di garantire l’applicazione e l’attuazione del presente regolamento. Le autorità nazionali competenti sono organizzate e gestite in modo che sia salvaguardata l’obiettività e l’imparzialità dei loro compiti e attività”.
Conclusioni
Le opzioni sul tavolo del decisore politico sono varie. Di certo sembra difficile prescindere, nella futura governance, da un coinvolgimento, ratione materiae, delle Autorità di protezione dati e dall’esigenza che le future Autorità europee di riferimento vantino un livello adeguato di effettiva indipendenza.
_________
Note
[1] Cfr. C. Casonato, Intelligenza artificiale e diritto costituzionale: prime considerazioni, in Dir. pubbl. comp. ed europeo, Speciale/2019, 101 ss.; A. Simoncini, L’algoritmo incostituzionale: intelligenza artificiale e il futuro delle libertà, in BioLaw Journal – Rivista di BioDiritto, 1, 2019, 69; T.E. Frosini, Il costituzionalismo nella società tecnologica, in Dir. inf. e inf., 2020, 465 ss.; T. Groppi, Alle frontiere dello Stato Costituzionale: innovazione tecnologica e intelligenza artificiale, in Consulta online, 3, 2020, 666 ss.; A. Simoncini e S. Suweis, Il cambio di paradigma nell’intelligenza artificiale e il suo impatto sul diritto costituzionale, in Rivista di filosofia del diritto, 1, 2019, 87 ss.
[2] Con riferimento specifico al processo amministrativo, cfr. P. Piras, Il processo amministrativo e l’innovazione tecnologica. Diritto al giusto processo versus intelligenza artificiale, in Dir. proc. amm., 2022, 997 ss.; M. Ramajoli (a cura di), Una giustizia amministrativa digitale?, Bologna, Il Mulino, 2023.
[3] Sul diverso approccio europeo e americano alla IA, cfr. E. Chiti e B. Marchetti, Divergenti? Le strategie di Unione europea e Stati Uniti in materia di intelligenza artificiale, in Riv. regolazione mercati, 2020, 29 ss. Cfr. anche P. Pajno, F. Donati, A. Perrucci (a cura di), Intelligenza artificiale e diritto: una rivoluzione? in particolare il vol. 1°, Diritti fondamentali, dati personali e regolazione, Bologna, Il Mulino, 2022.
[4] M. Bassini e O. Pollicino, Intelligenza artificiale e protezione dei dati personali, in www.astrid-online.it, 2022, fasc. 3.
[5] U. Ruffolo e A. Amidei, La regolazione ex ante dell’intelligenza artificiale tra gestione del rischio by design, strumenti di certificazione preventiva ed «autodisciplina» di settore, in www.astrid-online.it., 2022, fasc. 10.
[6] In dottrina, richiamato lo studio antesignano A. Predieri, Gli elaboratori elettronici nell’amministrazione dello Stato, Bologna, Il Mulino, 1971, si vedano i contributi più recenti, tra cui M.C. Cavallo, Amministrazione pubblica e sistemi di intelligenza artificiale: alcune riflessioni, in Dir. e processo amm., 2022, 955; V. Fanti, Intelligenza artificiale e diritto amministrativo: prospettive, limiti, pericoli, in Dir. e proc. amm.,2022, 1002.
[7] A. Cassatella, La discrezionalità amministrativa nell’età digitale, in AA.VV. (a cura di), Diritto amministrativo: scritti per Franco Gaetano Scoca, vol. 1°, Napoli. Edizione Scientifiche Italiane, 2021, 675 ss.
[8] I.M. Dalgado, Automazione, intelligenza artificiale e pubblica amministrazione: vecchie categorie concettuali per nuovi problemi? , in Istituzione del federalismo, 2019, 643 ss.
[9] G. Gallone, Riserva di umanità e funzioni amministrative, Padova, Cedam, 2023.
[10] Il Tribunale ha osservato che non è conforme al sistema di diritto amministrativo “affidare all’attivazione di meccanismi e sistemi informatici e al conseguente loro impersonale funzionamento, il dipanarsi di procedimenti amministrativi, sovente incidenti su interessi, se non diritti, di rilievo costituzionale, che invece postulano, onde approdare al corretto esito provvedimentale conclusivo” (TAR Lazio, sez. III bis, n. 9224 del 2018).
[11] S. Civitarese Matteucci, Umano troppo umano. Decisioni amministrative automatizzate e principio di legalità, in Dir. pubblico, 2019, 5 ss.
[12] In quest’ambito è significativo l’atto della Commissione Europea, Dichiarazione europea sui diritti e i principi digitali per il decennio digitale, 26 gennaio 2022, in cui si proclama l’impegno dell’Unione Europea a “garantire che le tecnologie come gli algoritmi e l’intelligenza artificiale non siano utilizzate per predeterminare le scelte delle persone, ad esempio per quanto riguarda la salute, l’istruzione, l’occupazione e la vita privata”. Ciò sul presupposto che “Ogni persona dovrebbe essere messa nelle condizioni di godere dei benefici offerti dall’intelligenza artificiale facendo le proprie scelte informate nell’ambiente digitale, e rimanendo al contempo protetta dai rischi e dai danni alla salute, alla sicurezza e ai diritti fondamentali”.
[13] Cfr. F. Cardarelli, Artt. 3 – 3 bis, in M.A. Sandulli (a cura di), Codice dell’azione amministrativa, Milano, Giuffré, 2017.
[14] Nel senso che la tecnologia deve essere solo un mezzo, cfr. A. Simoncini, L’algoritmo incostituzionale: intelligenza artificiale e il futuro delle libertà, cit. che parla di un soggetto “catturato dallo strumento” mettendo in evidenza come “la tecnologia, così come la tecnica, hanno sempre fatto parte della categoria dei mezzi” ma questo assetto logico-concettuale è messo in crisi dal “fatto che oggi la tecnologia […] non è più soltanto un mezzo per realizzare un corso di azioni deciso da un soggetto agente umano, ma sempre più spesso, è essa stessa a prendere decisioni rilevanti per la persona umana e la sua libertà”.
[15] Cfr. B. Marchetti, L’amministrazione digitale, in B.G. Mattarella e M. Ramajoli (a cura di), Enciclopedia del Diritto, I Tematici, Funzione amministrative, Milano Giuffré, 2022.
[16] E Carloni, I principi della legalità algoritmica. Le decisioni automatizzate di fronte al giudice amministrativo, in Dir. amm., 2020, 271 ss.
[17] Tar Lazio, Roma, sez. III, sentenza n. 4409 del 15 aprile 2021.
[18] Cons. St., sez. VI, 13 dicembre 2019, n. 8474, con commento di A.G. Orofino, G. Gallone, L’intelligenza artificiale al servizio delle funzioni amministrative: profili problematici e spunti di riflessione, in Giur. it., 2020, 1738.
[19] Per la tassonomia dei vari tipi di automazione, cfr. G. Gallone, Il consiglio di stato marca la distinzione tra algoritmo, automazione ed intelligenza artificiale (nota a Cons. Stato, sez. III, 25 novembre 2021, n. 7891), in Dir. Internet, 2022, 161.
[20] La locuzione è mutuata dallo scritto di A. Simoncini, L’algoritmo incostituzionale: intelligenza artificiale e il fututo delle libertà, cit.
[21] In letteratura cfr. L Fiorentino e A. La Chimia ( a cura di), Il “procurement” delle pubbliche amministrazioni. Tra innovazione e sostenibilità, Bologna, Il Mulino, 2021; G.M. Racca, La digitalizzazione dei contratti pubblici: adeguatezza delle pubbliche amministrazioni e qualificazione delle imprese, in R. Cavallo Perin e D.U. Galetta (a cura di), Il diritto dell’amministrazione pubblica digitale, Torino, Giappichelli, 2020, 321 ss.
[22] Sul Regolamento europeo in materia di IA si vedano: G. Finocchiaro, La regolazione dell’intelligenza artificiale, in Riv. trim. dir. pubbl., 2022, 1085 ss.; C. Silvano, Prospettive di regolazione della decisione amministrativa algoritmica: un’analisi comparata, in Riv. it. dir. pubbl. com., 2022, 265; C. Casonato e B. Marchetti, Prime osservazioni sulla proposta di regolamento dell’Unione europea in materia di intelligenza artificiale, in BioDiritto , 2021, 1 ss.; F Rodi, Gli interventi dell’Unione europea in materia di intelligenza artificiale e robotica: problemi e prospettive, in G. Alpa (a cura di), Diritto e intelligenza artificiale, Pisa, Pacini, 2020, pp. 187-210: G. Resta, Cosa c’è di “europeo” nella proposta di regolamento UE sull’intelligenza artificiale? , in Dir. Inf., 2022, p. 323 ss.
[23] L. Torchia, Lo Stato Digitale – Una Introduzione, Bologna, Il Mulino, 2023.

